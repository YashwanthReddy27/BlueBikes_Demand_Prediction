# BlueBikes_Demand_Prediction

## Project Overview

### Objective
Predict hourly bike rental demand at each BlueBikes station to optimize bike redistribution and ensure availability.

### Approach
1. **Data Collection**: BlueBikes 2020 trip data + Boston weather data
2. **Feature Enineering**: 41+ engineered feaures (temporal, spatial, historical, weather, demographic)
3. **Model Development**: Three regression approaches with hyperparmeter tuning
4. **Evaluation**: Time-series cross-validation with proper temporal splitting
5. **Deployment**: Interactive dashboard for real-time scenario analysis


## Results Summary

| Model | Test RMSE | Test MAE | Test R¬≤ |
|-------|-----------|----------|---------|
| Linear Regression (Polynomial) | 2.012 | 1.245 | 0.367 |
| Random Forest | 1.186 | 0.680 | 0.759 |
| **Neural Network** | **1.550** | **0.896** | **0.526** |

*Neural Network architecture: [128, 64, 32, 16] with 0.2 dropout*
**All the output files are in the results folder including the config and weights files which are also included in drive link**

## Environment setup
**conda env**
1. conda create --name "<env_name>" python=3.10python -m venv venv(for virtual environment) 
2. pip install - r requirements.txt

**Local env**
1. python -m venv venv(for virtual environment) then venv\Scripts\activate(windows) or source venv/bin/activate(macos/linux)

## 1. Data
**Drive_Link:** https://drive.google.com/drive/folders/1i4XQky2Q0RdkhUlO3_j2mjQkplYd44QB?usp=sharing
### Structure
```
‚îú‚îÄ‚îÄ Data/                          # Data directory (create this)
‚îÇ   ‚îú‚îÄ‚îÄ bluebikes_tripdata_2020.csv    # Raw trip data from kaggle
‚îÇ   ‚îú‚îÄ‚îÄ bluebikes_features_complete.csv # Data created for NN for pred   
‚îÇ   ‚îú‚îÄ‚îÄ boston_weather_2020.csv        # Weather data
‚îÇ   ‚îî‚îÄ‚îÄ bluebikes_ml_ready.csv       # Feature Engineered Dataset for all models
```

i) Raw Trip Data (`bluebikes_tripdata_2020.csv`)- This is the raw trip-level data containing every bike rental in 2020.

**Source:**
[
Kaggle BlueBikes Dataset
](
https://www.kaggle.com/datasets/jackdaoud/bluebikes-in-boston/data
)

ii) Weather Data (`boston_weather_2020.csv`) - Hourly weather conditions for Boston throughout 2020.

**Source:**
 Open-Meteo Historical Weather API

iii) ML-Ready Dataset (`bluebikes_ml_ready.csv`)- This is the primary dataset used for training all models. It aggregates trip data to station √ó hour level and adds engineered features.

**Source:**
 Generated by 
`feature_engineering.py`


**
Total Columns:
**
 42 (2 identifiers + 34 features + 1 target + 5 weather one-hot)
####
 Complete Column List

```

station_id, timestamp, hour_of_day_sin, hour_of_day_cos, day_of_week_sin, 
day_of_week_cos, month_sin, month_cos, is_weekend, is_peak_hour, is_holiday, 
special_event_flag, station_latitude, station_longitude, station_capacity, 
neighborhood_cluster_id, station_type, demand_t_minus_1, demand_t_minus_24, 
demand_t_minus_168, rolling_mean_7d, rolling_std_7d, same_hour_previous_week, 
month_to_date_average, day_of_week_average_4w, trend_coefficient_7d, temperature, 
feels_like_temperature, precipitation_mm, wind_speed_mph, weather_severity_score, 
subscriber_ratio, average_trip_duration, return_trip_probability, average_age_bracket, 
demand, weather_category, weather_severity_score, weather_clear, weather_heavy_rain, 
weather_hot, weather_rain, weather_windy

```

## 2. Pipeline Execution
Execute the pipeline in order. Each step depends on the previous one.


### Step 1: Exploratory Data Analysis (Optional)

```
bash

python eda.py

```

**What it does:**
- Loads raw trip data
- Analyzes demand patterns (hourly, daily, weekly)
- Identifies top stations
- Checks data quality
**Output:**
 Console analysis and recommendations
---

To get weather data
```
bash

python get_wather_data.py
```

### Step 2: Feature Engineering

```
bash

python feature_engineering.py

```

**What it does:**

- Loads trip data and weather data
- Creates 41+ engineered features:
    - **Temporal**: Cyclical hour/day/month encodings, weekend/holiday flags
    - **Spatial**: Station coordinates, neighborhood clusters, station type
    - **Historical**: Lag features (t-1, t-24, t-168), rolling means/std
    - **Weather**: Temperature, precipitation, severity scores
    - **Demographic**: Subscriber ratio, age bracket, trip duration

**Input files:**
- `Data/bluebikes_tripdata_2020.csv`
- `Data/boston_weather_2020.csv`

**Output file:**
- `Data/bluebikes_ml_ready.csv` (~800K records √ó 43 features)
---


### Step 3: Train Linear Regression Model

```
bash

python linear_regression.py

```

**What it does:**
- Loads ML-ready data (`bluebikes_ml_ready.csv`)
- Excludes non-numeric columns automatically
- Trains baseline model (degree-2 polynomial + Ridge, alpha=1.0)
- Performs GridSearchCV with TimeSeriesSplit (3-fold)
- Compares baseline vs optimized performance
- Saves feature importance analysis

**Configuration:**
- Train/Val/Test split: 70/15/15 (temporal)
- Polynomial degrees tested: [1, 2]
- Ridge alpha values: [0.01, 0.1, 1.0, 10.0]
- Cross-validation: 3-fold TimeSeriesSplit
- Memory optimization: float32 casting

**Output files:**
- `baseline_lr_poly_model.pkl` - Baseline model
- `optimized_lr_poly_model.pkl` - Tuned model
- `model_metrics_log.json` - Comprehensive metrics
- `feature_importance.csv` - Feature coefficients
---

### Step 4: Train Random Forest Model

**Option A: Using Jupyter Notebook (Recommended for Colab)**

```
bash

jupyter notebook ML_RandomForest.ipynb

```

Run all cells in order.

**Option B: Using Google Colab**

1. Upload `ML_RandomForest.ipynb` to Colab
2. Upload `bluebikes_ml_ready.csv`
3. Run all cells

**What it does:**
- Trains baseline Random Forest (n_estimators=100, max_depth=10)
- Tests 3 optimized configurations manually
- Uses 80/20 train/test split
- Compares baseline vs optimized performance
- Analyzes feature importance

**Parameter Sets Tested:**

```
python

param_sets = [   
{'n_estimators':50, 'max_depth': 20,'min_samples_split': 10},
{'n_estimators': 100, 'max_depth':15,'min_samples_split': 5},    
{'n_estimators': 50,'max_depth':None,'min_samples_split': 10,'max_features':0.5}
]

```

**Output files:**
- `baseline_random_forest.pkl`
- `optimized_random_forest.pkl`
---


### Step 5: Train Neural Network Model

```
bash

python dnn.py

```
**Important:** Update the `data_path` in the `main()` function before running:
```
python tuner = DNNHyperparameterTuner(data_path='Data/bluebikes_ml_ready.csv')

```

**For GPU acceleration (recommended):**

1. Upload `dnn.py` and `bluebikes_ml_ready.csv` to Google Colab
2. Enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)
3. Update the data path and run
**What it does:**
1. Loads and preprocesses data with categorical encoding
2. Creates temporal train/val/test split (70/15/15)
3. Trains baseline DNN ([128, 64, 32] layers, dropout=0.3)
4. Performs 5-fold time series cross-validation
5. Grid search over 16 hyperparameter combinations
6. Trains final model with best hyperparameters
7. Generates comprehensive visualizations
**Hyperparameter Grid:**

```
param_grid = {
    'layers_config': [[256, 128, 64], [128, 64, 32, 16]],
    'dropout_rate': [0.2, 0.3],
    'learning_rate': [0.001, 0.01],
    'batch_size': [256, 512]
}
```

**Training Features:**

- Early stopping (patience=10)
- Learning rate scheduling (ReduceLROnPlateau)
- Batch normalization
- He initialization for hidden layers
- Xavier initialization for output layer
- Incremental model saving

**Output files:**
- `dnn_baseline_config.pkl` / `dnn_baseline_weights.pkl`
- `dnn_best_config.pkl` / `dnn_best_weights.pkl`
- `dnn_cv_results.csv`
- `dnn_final_metrics.json`
- `pytorch_dnn_complete_results.png`
---


### Step 6: Generate Dashboard Predictions

**Before running, update file paths in `extract_data.py`:**

```
CONFIG = {
    'data_file': 'Data/bluebikes_ml_ready.csv',
    'linear_regression': 'results/configs_weights/optimized_lr_poly_model.pkl',
    'random_forest': 'results/configs_weights/optimized_random_forest.pkl',
    'neural_network_config': 'results/configs_weights/dnn_best_config.pkl',
    'neural_network_weights': 'results/configs_weights/dnn_best_weights.pkl',
    'test_size': 0.15
}
```

Then run:
```
bash

python extract_data.py

```

**What it does:**

- Loads all trained models
- Generates predictions on test set
- Calculates metrics for each model
- Creates dashboard-ready CSV files

**Output files:**
- `predictions_for_dashboard.csv`
- `model_metrics_for_dashboard.csv`
- `station_metadata_for_dashboard.csv`
---


1. Open `dashboard.html` in a web browser(or open with live server in vscode)
2. Upload the generated CSV files:
- **Predictions**: `predictions_for_dashboard.csv`
- **Model Metrics**: `model_metrics_for_dashboard.csv`
- **Station Metadata**: `station_metadata_for_dashboard.csv`(optional)

**Dashboard Features:**
- üå§Ô∏è Weather scenario  (rain, cold, sunny) and ‚è∞ Time period adjustments (rush hour, weekend, night) simulation
- Dynamic model switching
- üìà 24-hour demand forecast chart
- üèÜ Top stations by predicted demand
- üî• Hourly demand heatmap
- üéØ Model performance comparison


## Model Details Summary

### Linear Regression (Polynomial Features)
**Architecture:**
```

Pipeline: StandardScaler ‚Üí PolynomialFeatures ‚Üí Ridge Regression

```

**Hyperparameter Grid:**
| Parameter | Values Tested |
|-----------|---------------|
| `poly__degree` | [1, 2] |
| `ridge__alpha` | [0.01, 0.1, 1.0, 10.0] |

**Configuration:**
- Cross-validation: 3-fold TimeSeriesSplit
- Train/Val/Test split: 70/15/15 (temporal)
- Memory optimization: float32 casting
- Baseline: degree=2, alpha=1.0
**Output Files:**
- `baseline_lr_poly_model.pkl`
- `optimized_lr_poly_model.pkl`
- `model_metrics_log.json`
- `feature_importance.csv`
---

### Random Forest Regressor
**Baseline Configuration:**

```
python

RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)

```

**Hyperparameter Search (Manual Grid):**
| Config | n_estimators | max_depth | min_samples_split | max_features |
|--------|--------------|-----------|-------------------|--------------|
| 1 | 50 | 20 | 10 | - |
| 2 | 100 | 15 | 5 | - |
| 3 | 50 | None | 10 | 0.5 |

**Best Parameters Found:**
```
python

{'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10}

```

**Output Files:**
- `baseline_random_forest.pkl`
- `optimized_random_forest.pkl`
---

### Deep Neural Network (PyTorch)

**Architecture:**

```

Input(n_features) ‚Üí [Hidden Layers with BatchNorm + ReLU + Dropout] ‚Üí Output(1)

```

**Layer Structure:**

- Linear layer ‚Üí ReLU activation ‚Üí BatchNorm ‚Üí Dropout
- Output layer: Linear with Xavier initialization
- Hidden layers: He (Kaiming) initialization

**Hyperparameter Grid:**
| Parameter | Values Tested |
|-----------|---------------|
|`layers_config`| [[256, 128, 64], [128, 64, 32, 16]] |
| `dropout_rate` | [0.2, 0.3] |
| `learning_rate` | [0.001, 0.01] |
| `batch_size` | [256, 512] |

**Total Combinations:** 16 configurations √ó 5-fold CV = 80 model trainings
**Training Configuration:**
- Optimizer: Adam
- Loss: MSELoss
- Early stopping: patience=10
- Max epochs per config: 50
- Final training epochs: 100
- LR Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)

**Output Files:**
- `dnn_baseline_config.pkl` - Baseline model configuration
- `dnn_baseline_weights.pkl` - Baseline model weights
- `dnn_best_config.pkl` - Best model configuration + scaler parameters
- `dnn_best_weights.pkl` - Best model weights
- `dnn_cv_results.csv` - All cross-validation results
- `dnn_final_metrics.json` - Final performance metrics
- `pytoch_dnn_complete_results.png` - Visualization plots
---


## Acknowledgments
- Kaggle for providing open trip data for Bluebikes
- Open-Meteo for free weather API
